{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1d0c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import string\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c8a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07742001",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweet-preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc05bad",
   "metadata": {},
   "source": [
    "## **Konversi Ke CSV Dan Mengambil Kolom Teks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b288bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = os.listdir('raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6860492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : @Nicho_Silalahi_user_tweets.xlsx\n",
      "Dataset : @uttazt__user_tweets.xlsx\n",
      "Dataset : @bayu_joo_user_tweets.xlsx\n",
      "Dataset : @dwikimic_user_tweets.xlsx\n",
      "Dataset : @Mdy_Asmara1701_user_tweets.xlsx\n",
      "Dataset : @cursedkidd_user_tweets.xlsx\n",
      "Dataset : @pinotski_user_tweets.xlsx\n",
      "Dataset : @iirman__user_tweets.xlsx\n",
      "Dataset : @jek____user_tweets.xlsx\n",
      "Dataset : @ayubsr_user_tweets.xlsx\n",
      "Dataset : @ekowboy2_user_tweets.xlsx\n",
      "Dataset : @andihidayat_user_tweets.xlsx\n",
      "Dataset : @ainunrozi_user_tweets.xlsx\n",
      "Dataset : @handokotjung_user_tweets.xlsx\n",
      "Dataset : @hotradero_user_tweets.xlsx\n",
      "Dataset : @radenrauf_user_tweets.xlsx\n"
     ]
    }
   ],
   "source": [
    "for name in dataset_list:\n",
    "    if name != '.ipynb_checkpoints':\n",
    "        print('Dataset :', name)\n",
    "\n",
    "        df = pd.read_excel(f'raw/{name}')\n",
    "        df = df[['Text']]\n",
    "\n",
    "        path = name.split('.')\n",
    "        df.to_csv(f'processed_text_only/{path[0]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93d8a14",
   "metadata": {},
   "source": [
    "## **Preprocessing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8067de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kata_baku = pd.read_csv('https://raw.githubusercontent.com/ShinyQ/One-Click-Sentiment_BE/main/app/dataset/Kamu-Alay.csv')\n",
    "kata_baku = kata_baku.set_index(\"kataAlay\")[\"kataBaik\"].to_dict()\n",
    "\n",
    "stop_words = list(pd.read_csv('https://raw.githubusercontent.com/ShinyQ/E-Wallet-Sentiment-Analysis_IFest-Unpad-2021/main/dataset/processed/stopwords_id_satya.txt', header=None)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9f17a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(text):   \n",
    "    \n",
    "    text = p.clean(text)\n",
    "    \n",
    "    # Mengubah setiap kata menjadi lowercase\n",
    "    text =  text.lower() \n",
    "\n",
    "    # Menghapus Link Dengan Pattern http/https dan www\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub('(@\\w+|#\\w+)', '', text)\n",
    "\n",
    "    # Menghapus Tag HTML\n",
    "    text = re.sub('<.*?>', '', text)\n",
    "\n",
    "    # Menghapus Tanda Baca Seperti Titik Dan Koma\n",
    "    text = text.translate(str.maketrans(' ', ' ', string.punctuation))\n",
    "\n",
    "    # Menghapus Karakter Selain Huruf a-z dan A-Z\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Mengganti baris baru (enter) dengan spasi\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "\n",
    "    # Menghapus Karakter Berulang (Contoh: Horeeee!!!! menjadi Hore!)\n",
    "    text = re.sub(r'(\\w)(\\1{2,})', r\"\\1\", text)\n",
    "\n",
    "    # Menghapus 1 Karakter Terpisah\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "    # Menghapus Spasi Yang Lebih Dari Satu\n",
    "    text = re.sub('(s{2,})', ' ', text)\n",
    "    \n",
    "    # Menghapus kata yang mengandung judul topik dan kata yang terdapat pada stopwords indonesia\n",
    "    temp_text_split = []\n",
    "    text_split = text.split(' ')\n",
    "\n",
    "    for i in range(len(text_split)):\n",
    "        if text_split[i] not in stop_words:\n",
    "            if text_split[i] in kata_baku:\n",
    "                 text_split[i] = kata_baku[text_split[i]]\n",
    "\n",
    "            temp_text_split.append(str(text_split[i]))\n",
    "    \n",
    "    temp_text_split = list(set(temp_text_split))  \n",
    "    text = ' '.join(temp_text_split)\n",
    "       \n",
    "    # Mengembalikan Hasil Preprocessing Text\n",
    "    return str.lstrip(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1d38c1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = os.listdir('processed_text_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3c8bdece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : @bayu_joo_user_tweets.csv\n",
      "Total Data : 2503 \n",
      "\n",
      "Dataset : @cursedkidd_user_tweets.csv\n",
      "Total Data : 1637 \n",
      "\n",
      "Dataset : @iirman__user_tweets.csv\n",
      "Total Data : 2581 \n",
      "\n",
      "Dataset : @dwikimic_user_tweets.csv\n",
      "Total Data : 2044 \n",
      "\n",
      "Dataset : @uttazt__user_tweets.csv\n",
      "Total Data : 1861 \n",
      "\n",
      "Dataset : @ainunrozi_user_tweets.csv\n",
      "Total Data : 2423 \n",
      "\n",
      "Dataset : @pinotski_user_tweets.csv\n",
      "Total Data : 2494 \n",
      "\n",
      "Dataset : @hotradero_user_tweets.csv\n",
      "Total Data : 2741 \n",
      "\n",
      "Dataset : @Nicho_Silalahi_user_tweets.csv\n",
      "Total Data : 2334 \n",
      "\n",
      "Dataset : @handokotjung_user_tweets.csv\n",
      "Total Data : 2220 \n",
      "\n",
      "Dataset : @radenrauf_user_tweets.csv\n",
      "Total Data : 1841 \n",
      "\n",
      "Dataset : @ayubsr_user_tweets.csv\n",
      "Total Data : 1166 \n",
      "\n",
      "Dataset : @andihidayat_user_tweets.csv\n",
      "Total Data : 2872 \n",
      "\n",
      "Dataset : @ekowboy2_user_tweets.csv\n",
      "Total Data : 1107 \n",
      "\n",
      "Dataset : @jek____user_tweets.csv\n",
      "Total Data : 1686 \n",
      "\n",
      "Dataset : @Mdy_Asmara1701_user_tweets.csv\n",
      "Total Data : 2600 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name in dataset_list:\n",
    "    if name != '.ipynb_checkpoints':\n",
    "        print('Dataset :', name)\n",
    "\n",
    "        df = pd.read_csv(f'processed_text_only/{name}')\n",
    "        \n",
    "        df['Text'] = df['Text'].apply(cleansing)\n",
    "        df = df[df['Text'].str.split().str.len().gt(2)]\n",
    "        df.to_csv(f'processed_clean/{name}', index=False)\n",
    "        \n",
    "        print(f\"Total Data : {len(df)} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f046ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --id 1kcC3rB8ylGN5tlDH-RCkSIk7mBQ_rN-h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "575662f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Traits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>jangan sungkan mari berkunjung ke toko saya</td>\n",
       "      <td>extraversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>telah terjadi pembullyan kepada penyebar berit...</td>\n",
       "      <td>neuroticism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>ini kenapa komplain saya ditutup</td>\n",
       "      <td>neuroticism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>mampus ga tayang lagi film di bioskop</td>\n",
       "      <td>neuroticism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>buset pada jualan</td>\n",
       "      <td>neuroticism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text        Traits\n",
       "495         jangan sungkan mari berkunjung ke toko saya  extraversion\n",
       "1073  telah terjadi pembullyan kepada penyebar berit...   neuroticism\n",
       "584                    ini kenapa komplain saya ditutup   neuroticism\n",
       "290               mampus ga tayang lagi film di bioskop   neuroticism\n",
       "332                                 buset pada jualan     neuroticism"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Personality Dataset.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "37ae4917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dataset Latih : 983\n"
     ]
    }
   ],
   "source": [
    "df['Text'] = df['Text'].apply(cleansing)\n",
    "df = df[df['Text'].str.split().str.len().gt(2)]\n",
    "\n",
    "print(\"Total Dataset Latih :\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f94ab9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Personality Dataset Clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77239b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
